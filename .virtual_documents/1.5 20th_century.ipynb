


from textblob import TextBlob
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
import re
from collections import Counter

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Optional: Set Seaborn style
sns.set(style="whitegrid")





with open(r"C:\Users\carol\Documents\Data Specialization\20th_century\20th_century_Wiki.txt", "r",errors='ignore') as file:
    data = file.read().replace('\n', ' ')





import nltk
nltk.download('punkt_tab')

from nltk.tokenize import sent_tokenize
tokenized_sent = sent_tokenize(data)
print(tokenized_sent) 


# Tokenize the text into single words

from nltk.tokenize import word_tokenize
tokenized_word = word_tokenize(data)
print(tokenized_word)


# Analise words frequency distribution

from nltk.probability import FreqDist
dist_words = FreqDist(tokenized_word)
print (dist_words)


#Review the most common (10) words
dist_words.most_common(10)


common_words = dist_words.most_common(10)
words = [word for word, count in common_words]
counts = [count for word, count in common_words]


# Plot the bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x=counts, y=words, palette="viridis")
plt.title("Top 10 Most Common Words")
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.show()





# Defining stopwords

from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
print(stop_words)


filtered_words = [] # creates an empty list
for word in tokenized_word:
    if word not in stop_words:
        filtered_words.append(word)


# Create a new FreqDist for filtered_words

dist_words_filter = FreqDist(stop_words)
print(dist_words_filter)


filtered_common_words = dist_words_filter.most_common(10)
words = [word for word, count in filtered_common_words]
counts = [count for word, count in filtered_common_words]


plt.figure(figsize=(10, 6))
sns.barplot(x=counts, y=words, palette="viridis")
plt.title("Top 10 Most Common Words (After Removing Stop Words and Punctuation)")
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.show()





# Substitute all punctuation marks with a space

sans_punc = re.sub("[^a-zA-Z]",  # Search for all non-letters
                          " ",          # Replace all non-letters with spaces
                          str(filtered_words))


#Checking data

sans_punc


#create another tokenized object 

tokenized_word_2 = word_tokenize(sans_punc)
print(tokenized_word_2)


# Create a new FreqDist

dist_words_filter_2 = FreqDist(tokenized_word_2)


# Get the 19 most common words after further processing
filtered_common_words = dist_words_filter_2.most_common(19)
words = [word for word, count in filtered_common_words]
counts = [count for word, count in filtered_common_words]



# Plot the bar chart
plt.figure(figsize=(12, 8))
sns.barplot(x=counts, y=words, palette="viridis")
plt.title("Top 19 Most Common Words (After Removing Punctuation and Further Cleaning)")
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.show()





new_stopwords = ["And", "Then", 'n', 't', 's', 'The']


filtered = []
for word in tokenized_word_2:
    if word not in new_stopwords:
        filtered.append(word)


%%time 
text = TextBlob(str(filtered))


#Check object text
text


# Create a tag list ( describe whether a word is a noun , adjective, verb...)
import nltk
nltk.download('averaged_perceptron_tagger_eng')
tags_list = text.tags


#Check object tag_list
tags_list


#Create a Dataframe from tag_list

df_text = pd.DataFrame(tags_list)
df_text.columns = ['Words', "Word type"]


df_text.head()


df_t = df_text.groupby('Word type').count().reset_index()


df_t.head()


top20 = df_t.nlargest(20, 'Words')


plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 20):
    sns.barplot(x = "Words", y = "Word type",
    saturation = 0.9, data = top20).set_title("20th_century - top 20 word types used")








df = df_text[(df_text['Word type'] == "NN") | (df_text['Word type'] == "NNS") | (df_text['Word type'] == "NNP")]
df.columns = ["Word", "Occurences"]
x = df.groupby('Word').count().reset_index()
y = x.sort_values(by = ['Occurences'], ascending=False)
top15 = y.nlargest(15, 'Occurences')


#  Check top15 nouns

top15


#Create a bar plot to visualize results. 

plt.figure(figsize=(15, 5))
with sns.dark_palette("xkcd:blue", 15):
    sns.barplot(x="Word", y="Occurences",
    saturation=0.9, data = top15).set_title("20th_century - most frequently used nouns")





df = df_text[(df_text['Word type'] == "VB")  | (df_text['Word type'] == "VBD")]
df.columns = ["Word", "Occurences"]
x = df.groupby('Word').count().reset_index()
y = x.sort_values(by = ['Occurences'], ascending=False)
top15 = y.nlargest(15, 'Occurences')


#Check top15 data

top15


plt.figure(figsize = (15, 5))
with sns.dark_palette("xkcd:blue", 15):
    sns.barplot(x = "Word", y = "Occurences",
    saturation = 0.9, data = top15).set_title("20th_century - most frequently used verbs")





df = df_text[df_text['Word type'] == "JJ"]
df.columns = ["Word", "Occurences"]
x = df.groupby('Word').count().reset_index()
y = x.sort_values(by=['Occurences'], ascending=False)
top15= y.nlargest(15, 'Occurences')


#Check data

top15


plt.figure(figsize=(15, 5))
with sns.dark_palette("xkcd:blue", 15):
    sns.barplot(x="Word", y="Occurences",
    saturation=0.9, data=top15).set_title("20th_century - most frequently used adjectives")








# Load country data
with open(r"C:\Users\carol\Documents\Data Specialization\20th_century\country_list2.txt", "r",errors='ignore') as file:
    country_list = file.read().splitlines()



country_list


# Step 2: Count country mentions in the text
from collections import Counter


# Convert the tokenized words to lowercase for matching
tokenized_words_lower = [word.lower() for word in tokenized_word]  # Use your existing tokenized words list
country_mentions = Counter()

for country in country_list:
    count = tokenized_words_lower.count(country.lower())
    if count > 0:
        country_mentions[country] = count



# Step 3: Create DataFrame
import pandas as pd

df_countries = pd.DataFrame(country_mentions.items(), columns=['Country', 'Mentions'])
df_countries = df_countries.sort_values(by='Mentions', ascending=False)



# Step 4: Plot the frequency of country mentions
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.barplot(data=df_countries.head(50), x='Mentions', y='Country', palette='dark')
plt.title('Top 20 Most Frequently Mentioned Countries')
plt.xlabel('Number of Mentions')
plt.ylabel('Country')
plt.show()


# Display the DataFrame for verification
df_countries.head(20)









